{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter optimization via continuation\n",
    "\n",
    "In this Notebook we explore several variations of a continuation approach for hyper-parameter optimization of meta-heuristic algorithms using TPE as hyper-parameter optimization methods for configuring PSO hyper-parameters on a benchmarck set of optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "import os.path\n",
    "import math\n",
    "import pickle\n",
    "import os.path\n",
    "import logging\n",
    "import ConfigSpace as CS\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from hyperopt import hp\n",
    "from multiprocessing import Pool, Lock\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import chpo.chpo as chpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Save and load partial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info():\n",
    "    info = json.load(open(\"data/prep/list.json\"))\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = dataset_info()\n",
    "datasets = {}\n",
    "\n",
    "def load_dataset(problem):\n",
    "    return datasets[problem['id']]\n",
    "\n",
    "def true_load_dataset(problem):\n",
    "    df = pd.read_csv('data/prep/' + problem['id'], header=0)\n",
    "    columns = df.columns\n",
    "    \n",
    "    df_train = df.sample(frac=0.6)\n",
    "    df_test = df.drop(df_train.index)\n",
    "    df_val = df_test.sample(frac=0.5)\n",
    "    df_test = df_test.drop(df_val.index)\n",
    "\n",
    "    Xt = df_train[columns[:-1]].values\n",
    "    Xv = df_val[columns[:-1]].values\n",
    "    X = df_test[columns[:-1]].values\n",
    "\n",
    "    yt = df_train[columns[-1]].values.ravel()\n",
    "    yv = df_val[columns[-1]].values.ravel()\n",
    "    y = df_test[columns[-1]].values.ravel()\n",
    "\n",
    "    return Xt, yt, Xv, yv, X, y\n",
    "\n",
    "for problem in problems:\n",
    "    datasets[problem['id']] = true_load_dataset(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = './results/nn-bench/'\n",
    "\n",
    "def save(name, config, data):\n",
    "    filename = name.join([str(x) for x in config])\n",
    "    with open(rdir + filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "        \n",
    "def load(name, config):\n",
    "    filename = name.join([str(x) for x in config])\n",
    "    \n",
    "    if os.path.isfile(rdir + filename):\n",
    "        if os.path.getsize(rdir + filename) == 0:\n",
    "            return False\n",
    "        with open(rdir + filename, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            return data\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. HPO problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Objective and search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpo_objective(param, budget, problem, phase):\n",
    "    Xt, yt, Xv, yv, X, y = load_dataset(problem)\n",
    "\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(int(param['n_hidden_1']), int(param['n_hidden_2'])),\n",
    "                       solver='sgd',\n",
    "                       batch_size=int(param['batch_size']),\n",
    "                       learning_rate_init=param['Lr'], \n",
    "                       momentum=param['M'])\n",
    "    \n",
    "    for i in range(int(budget)):\n",
    "        mlp.partial_fit(Xt, yt)\n",
    "        \n",
    "    if phase == 'test':\n",
    "        return mean_squared_error(y, mlp.predict(X))\n",
    "    else:\n",
    "        return mean_squared_error(yv, mlp.predict(Xv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M, Type: UniformFloat, Range: [0.8, 0.9999], Default: 0.89995"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperopt_space = {'n_hidden_1': hp.quniform('n_hidden_1', 1, 40, 1),\n",
    "                  'n_hidden_2': hp.quniform('n_hidden_2', 1, 40, 1),\n",
    "                  'batch_size': hp.quniform('batch_size', 32, 256, 10),\n",
    "                  'Lr': hp.uniform('Lr', 0.00001, 0.001),\n",
    "                  'M': hp.uniform('M', 0.8, 0.9999)}\n",
    "\n",
    "hb_space = CS.ConfigurationSpace()\n",
    "hb_space.add_hyperparameter(CS.UniformFloatHyperparameter('n_hidden_1', lower=1, upper=40))\n",
    "hb_space.add_hyperparameter(CS.UniformFloatHyperparameter('n_hidden_2', lower=1, upper=40))\n",
    "hb_space.add_hyperparameter(CS.UniformFloatHyperparameter('batch_size', lower=32, upper=256))\n",
    "hb_space.add_hyperparameter(CS.UniformFloatHyperparameter('Lr', lower=0.00001, upper=0.001))\n",
    "hb_space.add_hyperparameter(CS.UniformFloatHyperparameter('M', lower=0.8, upper=0.9999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HPO algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tpe(n_parent, n_base, problem):  \n",
    "    tpe = chpo.TPE(hyperopt_space, hpo_objective, problem)\n",
    "    return tpe.run(n_parent, n_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CTPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ctpe(n_parent, n_base, problem, buckets):\n",
    "    ctpe = chpo.CTPE(hyperopt_space, hpo_objective, problem, buckets)\n",
    "    return ctpe.run(n_parent, n_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hb(n_parent, n_base, problem, eta):\n",
    "    return chpo.hb(hb_space, hpo_objective, problem, n_parent, n_base, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. BOHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bohb(n_parent, n_base, problem, eta):\n",
    "    return chpo.bohb(hb_space, hpo_objective, problem, n_parent, n_base, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = list(range(10))\n",
    "buckets = [2, 3, 4, 5]\n",
    "\n",
    "n_base = 100\n",
    "n_parent = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 7348.54it/s]\n"
     ]
    }
   ],
   "source": [
    "iters = list(itertools.product(problems, trials))\n",
    "\n",
    "def tpe_run_config(config):\n",
    "    problem, trial = config\n",
    "    if not load('tpe', (problem['name'], trial)):\n",
    "        closs=run_tpe(n_parent, n_base, problem)\n",
    "        save('tpe', (problem['name'], trial), (closs))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    with Pool() as pool:\n",
    "        for _ in tqdm(pool.imap(tpe_run_config, iters), total=len(iters)):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. CTPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:00<00:00, 23382.34it/s]\n"
     ]
    }
   ],
   "source": [
    "iters = list(itertools.product(problems, buckets, trials))\n",
    "\n",
    "def ctpe_run_config(config):\n",
    "    problem, bucket, trial = config\n",
    "    if not load(\"ctpe\", (problem['name'], bucket, trial)):\n",
    "        closs = run_ctpe(n_parent, n_base, problem, bucket)\n",
    "        save(\"ctpe\", ((problem['name'], bucket, trial)), (closs))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool() as pool:\n",
    "        for _ in tqdm(pool.imap_unordered(ctpe_run_config, iters), total=len(iters)):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [3:25:43<00:00, 51.43s/it]  \n"
     ]
    }
   ],
   "source": [
    "iters = list(itertools.product(problems, buckets, trials))\n",
    "\n",
    "def hb_run_config(config):\n",
    "    problem, eta, trial = config\n",
    "    if not load(\"hb\", ((problem['name'], eta, trial))):\n",
    "        n_hb_parent = n_parent / (np.floor(math.log(n_base, eta)) + 1)\n",
    "        closs = run_hb(n_hb_parent, n_base, problem, eta)\n",
    "        save(\"hb\", ((problem['name'], eta, trial)), (closs))\n",
    "\n",
    "for config in tqdm(iters):\n",
    "    hb_run_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. BOHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [6:58:31<00:00, 104.63s/it]  \n"
     ]
    }
   ],
   "source": [
    "iters = list(itertools.product(problems, buckets, trials))\n",
    "\n",
    "def bohb_run_config(config):\n",
    "    problem, eta, trial = config\n",
    "    if not load(\"bohb\", ((problem['name'], eta, trial))):\n",
    "        n_hb_parent = n_parent / (np.floor(math.log(n_base, eta)) + 1)\n",
    "        closs = run_bohb(n_hb_parent, n_base, problem, eta)\n",
    "        save(\"bohb\", ((problem['name'], eta, trial)), (closs))\n",
    "\n",
    "for config in tqdm(iters):\n",
    "    bohb_run_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collecting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_names = [problem['name'] for problem in problems]\n",
    "\n",
    "def collect_tpe():\n",
    "    row_idx = itertools.product(problem_names)\n",
    "    column_idx = itertools.product(trials)\n",
    "    \n",
    "    idx = pd.MultiIndex.from_tuples(row_idx, names = ['problem'])\n",
    "    cols = pd.MultiIndex.from_tuples(column_idx, names = ['trial'])\n",
    "    iterspace = list(itertools.product(problem_names, trials))\n",
    "    df_loss = pd.DataFrame(index=idx, columns=cols)\n",
    "    \n",
    "    for iteration in iterspace:\n",
    "        problem, trial = iteration\n",
    "        index = (problem, trial)\n",
    "        \n",
    "        df_loss.loc[index] = load('tpe', index)\n",
    "\n",
    "    return df_loss\n",
    "\n",
    "def collect_ctpe():\n",
    "    row_idx = itertools.product(trials)\n",
    "    column_idx = itertools.product(problem_names, buckets)\n",
    "\n",
    "    idx = pd.MultiIndex.from_tuples(row_idx, names = ['trial'])\n",
    "    cols = pd.MultiIndex.from_tuples(column_idx, names = ['problem', 'bucket'])\n",
    "    iterspace = list(itertools.product(problem_names, buckets, trials))\n",
    "    df_loss = pd.DataFrame(index=idx, columns=cols)\n",
    "    \n",
    "    for iteration in iterspace:\n",
    "        problem, bucket, trial = iteration\n",
    "        index = ((trial), (problem, bucket))\n",
    "\n",
    "        df_loss.loc[index] = load('ctpe', ((problem, bucket, trial)))\n",
    "\n",
    "    return df_loss\n",
    "\n",
    "def collect_hb():\n",
    "    row_idx = itertools.product(trials)\n",
    "    column_idx = itertools.product(problem_names, buckets)\n",
    "\n",
    "    idx = pd.MultiIndex.from_tuples(row_idx, names = ['trial'])\n",
    "    cols = pd.MultiIndex.from_tuples(column_idx, names = ['problem', 'bucket'])\n",
    "    iterspace = list(itertools.product(problem_names, buckets, trials))\n",
    "    df_loss = pd.DataFrame(index=idx, columns=cols)\n",
    "    \n",
    "    for iteration in iterspace:\n",
    "        problem, bucket, trial = iteration\n",
    "        index = ((trial), (problem, bucket))\n",
    "\n",
    "        df_loss.loc[index] = load('hb', ((problem, bucket, trial)))\n",
    "        \n",
    "    return df_loss\n",
    "\n",
    "def collect_bohb():\n",
    "    row_idx = itertools.product(trials)\n",
    "    column_idx = itertools.product(problem_names, buckets)\n",
    "\n",
    "    idx = pd.MultiIndex.from_tuples(row_idx, names = ['trial'])\n",
    "    cols = pd.MultiIndex.from_tuples(column_idx, names = ['problem', 'bucket'])\n",
    "    iterspace = list(itertools.product(problem_names, buckets, trials))\n",
    "    df_loss = pd.DataFrame(index=idx, columns=cols)\n",
    "    \n",
    "    for iteration in iterspace:\n",
    "        problem, bucket, trial = iteration\n",
    "        index = ((trial), (problem, bucket))\n",
    "\n",
    "        df_loss.loc[index] = load('bohb', ((problem, bucket, trial)))\n",
    "        \n",
    "    return df_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpe_loss = collect_tpe()\n",
    "df_ctpe_loss = collect_ctpe()\n",
    "df_hb_loss = collect_hb()\n",
    "df_bohb_loss = collect_bohb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpe_loss.to_csv(rdir + 'tpe.loss.csv')\n",
    "df_ctpe_loss.to_csv(rdir + 'ctpe.loss.csv')\n",
    "df_hb_loss.to_csv(rdir + 'hb.loss.csv')\n",
    "df_bohb_loss.to_csv(rdir + 'bohb.loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
