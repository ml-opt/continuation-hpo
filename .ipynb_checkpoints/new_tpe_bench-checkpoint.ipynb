{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter optimization via continuation\n",
    "\n",
    "In this Notebook we explore several variations of a continuation approach for hyper-parameter optimization of meta-heuristic algorithms using TPE as hyper-parameter optimization methods for configuring PSO hyper-parameters on a benchmarck set of optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os.path\n",
    "import math\n",
    "import pickle\n",
    "import os.path\n",
    "import logging\n",
    "import ConfigSpace as CS\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from hyperopt import hp\n",
    "from multiprocessing import Pool, Lock\n",
    "\n",
    "import chpo.chpo as chpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Save and load partial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = './results/pso-benchmark/'\n",
    "\n",
    "def save(name, config, data):\n",
    "    filename = name.join([str(x) for x in config])\n",
    "    with open(rdir + filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "        \n",
    "def load(name, config):\n",
    "    filename = name.join([str(x) for x in config])\n",
    "    \n",
    "    if os.path.isfile(rdir + filename):\n",
    "        if os.path.getsize(rdir + filename) == 0:\n",
    "            return False\n",
    "        with open(rdir + filename, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            return data\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Interface to `dnn_opt` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pso_population = 40\n",
    "pso_problem_dim = 50\n",
    "pso_algorithm_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. PSO objective and search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dnn_opt(max_eval, problem, G, L, Mi, Ma):\n",
    "    result = subprocess.run(['dnn_opt/bin/examples/benchmark/benchmark', \n",
    "                             '-o', '1', '-n', str(pso_problem_dim), '-p', str(pso_population), \n",
    "                             '-eta', str(max_eval), '-a', str(pso_algorithm_idx), '-s', str(problem), \n",
    "                             '-ha', str(G), '-hb', str(L), '-hc', str(Mi), '-hd', str(Ma)], stdout=subprocess.PIPE)\n",
    "    return float(result.stdout)\n",
    "\n",
    "def pso_objective(param, budget, problem):\n",
    "    return run_dnn_opt(budget, problem, param['G'], param['L'], param['Mi'], param['Ma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ma, Type: UniformFloat, Range: [0.0, 3.0], Default: 1.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperopt_space = {'G': hp.uniform('G', 0, 3), 'L': hp.uniform('L', 0, 3), \n",
    "                  'Mi': hp.uniform('Mi', 0, 3), 'Ma': hp.uniform('Ma', 0, 3)}\n",
    "hb_space = CS.ConfigurationSpace()\n",
    "hb_space.add_hyperparameter(CS.UniformFloatHyperparameter('G', lower=0, upper=3))\n",
    "hb_space.add_hyperparameter(CS.UniformFloatHyperparameter('L', lower=0, upper=3))\n",
    "hb_space.add_hyperparameter(CS.UniformFloatHyperparameter('Mi', lower=0, upper=3))\n",
    "hb_space.add_hyperparameter(CS.UniformFloatHyperparameter('Ma', lower=0, upper=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HPO algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tpe(n_parent, n_base, problem):  \n",
    "    tpe = chpo.TPE(hyperopt_space, pso_objective, problem)\n",
    "    return tpe.run(n_parent, n_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CTPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ctpe(n_parent, n_base, problem, buckets):\n",
    "    ctpe = chpo.CTPE(hyperopt_space, pso_objective, problem, buckets)\n",
    "    return ctpe.run(n_parent, n_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hb(n_parent, n_base, problem, eta):\n",
    "    return chpo.hb(hb_space, pso_objective, problem, n_parent, n_base, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. BOHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bohb(n_parent, n_base, problem, eta):\n",
    "    return chpo.bohb(hb_space, pso_objective, problem, n_parent, n_base, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = list(range(0, 17)) \n",
    "trials = list(range(10))\n",
    "buckets = [2, 3, 4, 5]\n",
    "\n",
    "n_base = 1000\n",
    "n_parent = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [02:24<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "iters = list(itertools.product(problems, trials))\n",
    "\n",
    "def tpe_run_config(config):\n",
    "    problem, trial = config\n",
    "    if not load('tpe', (problem, trial)):\n",
    "        closs=run_tpe(n_parent, n_base, problem)\n",
    "        save('tpe', (problem, trial), (closs))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    with Pool() as pool:\n",
    "        for _ in tqdm(pool.imap(tpe_run_config, iters), total=len(iters)):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. CTPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [2:28:36<00:00, 13.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "iters = list(itertools.product(problems, buckets, trials))\n",
    "\n",
    "def ctpe_run_config(config):\n",
    "    problem, bucket, trial = config\n",
    "    if not load(\"ctpe\", (problem, bucket, trial)):\n",
    "        closs = run_ctpe(n_parent, n_base, problem, bucket)\n",
    "        save(\"ctpe\", ((problem, bucket, trial)), (closs))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool() as pool:\n",
    "        for _ in tqdm(pool.imap_unordered(ctpe_run_config, iters), total=len(iters)):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [14:09:23<00:00, 74.95s/it]   \n"
     ]
    }
   ],
   "source": [
    "iters = list(itertools.product(problems, buckets, trials))\n",
    "\n",
    "def hb_run_config(config):\n",
    "    problem, eta, trial = config\n",
    "    if not load(\"hb\", ((problem, eta, trial))):\n",
    "        n_hb_parent = n_parent / (np.floor(math.log(n_base, eta)) + 1)**2\n",
    "        closs = run_hb(n_hb_parent, n_base, problem, eta)\n",
    "        save(\"hb\", ((problem, eta, trial)), (closs))\n",
    "\n",
    "for config in tqdm(iters):\n",
    "    hb_run_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. BOHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [34:04:08<00:00, 180.37s/it]   \n"
     ]
    }
   ],
   "source": [
    "iters = list(itertools.product(problems, buckets, trials))\n",
    "\n",
    "def bohb_run_config(config):\n",
    "    problem, eta, trial = config\n",
    "    if not load(\"bohb\", ((problem, eta, trial))):\n",
    "        n_hb_parent = n_parent / (np.floor(math.log(n_base, eta)) + 1)**2\n",
    "        closs = run_bohb(n_hb_parent, n_base, problem, eta)\n",
    "        save(\"bohb\", ((problem, eta, trial)), (closs))\n",
    "\n",
    "for config in tqdm(iters):\n",
    "    bohb_run_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collecting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tpe():\n",
    "    row_idx = itertools.product(problems)\n",
    "    column_idx = itertools.product(trials)\n",
    "    \n",
    "    idx = pd.MultiIndex.from_tuples(row_idx, names = ['problem'])\n",
    "    cols = pd.MultiIndex.from_tuples(column_idx, names = ['trial'])\n",
    "    iterspace = list(itertools.product(problems, trials))\n",
    "    df_loss = pd.DataFrame(index=idx, columns=cols)\n",
    "    \n",
    "    for iteration in iterspace:\n",
    "        problem, trial = iteration\n",
    "        index = (problem, trial)\n",
    "        \n",
    "        df_loss.loc[index] = load('tpe', index)\n",
    "\n",
    "    return df_loss\n",
    "\n",
    "def collect_ctpe():\n",
    "    row_idx = itertools.product(trials)\n",
    "    column_idx = itertools.product(problems, buckets)\n",
    "\n",
    "    idx = pd.MultiIndex.from_tuples(row_idx, names = ['trial'])\n",
    "    cols = pd.MultiIndex.from_tuples(column_idx, names = ['problem', 'bucket'])\n",
    "    iterspace = list(itertools.product(problems, buckets, trials))\n",
    "    df_loss = pd.DataFrame(index=idx, columns=cols)\n",
    "    \n",
    "    for iteration in iterspace:\n",
    "        problem, bucket, trial = iteration\n",
    "        index = ((trial), (problem, bucket))\n",
    "\n",
    "        df_loss.loc[index] = load('ctpe', ((problem, bucket, trial)))\n",
    "\n",
    "    return df_loss\n",
    "\n",
    "def collect_hb():\n",
    "    row_idx = itertools.product(trials)\n",
    "    column_idx = itertools.product(problems, buckets)\n",
    "\n",
    "    idx = pd.MultiIndex.from_tuples(row_idx, names = ['trial'])\n",
    "    cols = pd.MultiIndex.from_tuples(column_idx, names = ['problem', 'bucket'])\n",
    "    iterspace = list(itertools.product(problems, buckets, trials))\n",
    "    df_loss = pd.DataFrame(index=idx, columns=cols)\n",
    "    \n",
    "    for iteration in iterspace:\n",
    "        problem, bucket, trial = iteration\n",
    "        index = ((trial), (problem, bucket))\n",
    "\n",
    "        df_loss.loc[index] = load('hb', ((problem, bucket, trial)))\n",
    "        \n",
    "    return df_loss\n",
    "\n",
    "def collect_bohb():\n",
    "    row_idx = itertools.product(trials)\n",
    "    column_idx = itertools.product(problems, buckets)\n",
    "\n",
    "    idx = pd.MultiIndex.from_tuples(row_idx, names = ['trial'])\n",
    "    cols = pd.MultiIndex.from_tuples(column_idx, names = ['problem', 'bucket'])\n",
    "    iterspace = list(itertools.product(problems, buckets, trials))\n",
    "    df_loss = pd.DataFrame(index=idx, columns=cols)\n",
    "    \n",
    "    for iteration in iterspace:\n",
    "        problem, bucket, trial = iteration\n",
    "        index = ((trial), (problem, bucket))\n",
    "\n",
    "        df_loss.loc[index] = load('bohb', ((problem, bucket, trial)))\n",
    "        \n",
    "    return df_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpe_loss = collect_tpe()\n",
    "df_ctpe_loss = collect_ctpe()\n",
    "df_hb_loss = collect_hb()\n",
    "df_bohb_loss = collect_bohb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpe_loss.to_csv(rdir + 'tpe.loss.csv')\n",
    "df_ctpe_loss.to_csv(rdir + 'ctpe.loss.csv')\n",
    "df_hb_loss.to_csv(rdir + 'hb.loss.csv')\n",
    "df_bohb_loss.to_csv(rdir + 'bohb.loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
